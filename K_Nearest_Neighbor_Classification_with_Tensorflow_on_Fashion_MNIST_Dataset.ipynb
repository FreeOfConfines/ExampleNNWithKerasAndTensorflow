{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K-Nearest Neighbor Classification with Tensorflow on Fashion MNIST Dataset",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FreeOfConfines/ExampleNNWithKerasAndTensorflow/blob/master/K_Nearest_Neighbor_Classification_with_Tensorflow_on_Fashion_MNIST_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jVWQHXuc1tvK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# K-Nearest Neighbor Classification with Python/Numpy/Tensorflow on Fashion MNIST Dataset"
      ]
    },
    {
      "metadata": {
        "id": "C0naFCFr1mam",
        "colab_type": "toc"
      },
      "cell_type": "markdown",
      "source": [
        ">[K-Nearest Neighbor Classification with Python/Numpy/Tensorflow on Fashion MNIST Dataset](#updateTitle=true&folderId=1PIZOzXSYokZJyrV92mRbYGRQMq-w7swD&scrollTo=jVWQHXuc1tvK)\n",
        "\n",
        ">>[So Far](#updateTitle=true&folderId=1PIZOzXSYokZJyrV92mRbYGRQMq-w7swD&scrollTo=QNWAWfSm2jLD)\n",
        "\n",
        ">>[What is Fashion MNIST Dataset?](#updateTitle=true&folderId=1PIZOzXSYokZJyrV92mRbYGRQMq-w7swD&scrollTo=nz2S4Mq_4T6P)\n",
        "\n",
        ">>[K-Nearest Neighbor Algorithm](#updateTitle=true&folderId=1PIZOzXSYokZJyrV92mRbYGRQMq-w7swD&scrollTo=uKUPyeHCfQU3)\n",
        "\n",
        ">>>[Closeness Metric](#updateTitle=true&folderId=1PIZOzXSYokZJyrV92mRbYGRQMq-w7swD&scrollTo=uKUPyeHCfQU3)\n",
        "\n",
        ">>>[Finding $k$ Closest Vectors in Train Set](#updateTitle=true&folderId=1PIZOzXSYokZJyrV92mRbYGRQMq-w7swD&scrollTo=uKUPyeHCfQU3)\n",
        "\n",
        ">>>[Optimizing Parameter $k$](#updateTitle=true&folderId=1PIZOzXSYokZJyrV92mRbYGRQMq-w7swD&scrollTo=uKUPyeHCfQU3)\n",
        "\n",
        ">>[Classification using Euclidean Distance Metric](#updateTitle=true&folderId=1PIZOzXSYokZJyrV92mRbYGRQMq-w7swD&scrollTo=g4TFt0SCdek4)\n",
        "\n",
        ">>>[Complexity](#updateTitle=true&folderId=1PIZOzXSYokZJyrV92mRbYGRQMq-w7swD&scrollTo=g4TFt0SCdek4)\n",
        "\n",
        ">>[Classification using L0 Distance](#updateTitle=true&folderId=1PIZOzXSYokZJyrV92mRbYGRQMq-w7swD&scrollTo=zWzeuu_0UPEE)\n",
        "\n",
        ">>>[Complexity](#updateTitle=true&folderId=1PIZOzXSYokZJyrV92mRbYGRQMq-w7swD&scrollTo=zWzeuu_0UPEE)\n",
        "\n",
        ">>>[Python-like Implementation](#updateTitle=true&folderId=1PIZOzXSYokZJyrV92mRbYGRQMq-w7swD&scrollTo=TfFce2zpJFuq)\n",
        "\n",
        ">>>[Tensorflow-like Implementation](#updateTitle=true&folderId=1PIZOzXSYokZJyrV92mRbYGRQMq-w7swD&scrollTo=VwhfUWFnJasE)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QNWAWfSm2jLD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##So Far\n",
        "\n",
        "In Part-2, we had designed, trained and tested a back-propogation network on Fashion MNIST dataset. Using a two layer backprop network designed using Keras and Tensorflow, we achieved a classification accuracy of 87.2%. In this article, we will revisit the classification (or labeling) problem on this dataset but apply a classification algorithm called the K-Nearest Neighbor (or KNN). We will show that KNN achieves classification accuracy only a little worse than the backprop network."
      ]
    },
    {
      "metadata": {
        "id": "nz2S4Mq_4T6P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What is Fashion MNIST Dataset?\n",
        "\n",
        "Fashion MNIST is a dataset of images that is given one of 10 unique labels like Tops, Trousers, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle Boot. The dataset is divided into two groups: Training Set and Test Set; there are 60000 images in Training Set and 10000 images in Test set. Each image is a $28 \\times 28$ array with values from 0 to 255."
      ]
    },
    {
      "metadata": {
        "id": "chIEsv7XyQp-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fJmYBGUzy7ss",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "cca301f9-eee7-49e8-d267-30c5075a2abc"
      },
      "cell_type": "code",
      "source": [
        "# Download Fashion MNIST dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(trImages, trLabels), (tImages, tLabels) = fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WEy4WAsBzFVo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "33c189c7-ee3e-4692-9197-f6db42848621"
      },
      "cell_type": "code",
      "source": [
        "print(\"--------------------------\")\n",
        "print(\"Dimensions of Train Set\")\n",
        "print(\"Dimension(trImages)=\",np.shape(trImages))\n",
        "print(\"There are\", np.shape(trImages)[0], \"images where each image is\", np.shape(trImages)[1:], \"in size\")\n",
        "print(\"There are\", np.shape(np.unique(tLabels))[0], \"unique image labels\")\n",
        "print(\"--------------------------\")\n",
        "print(\"Dimensions of Test Set\")\n",
        "print(\"Dimension(tImages)=\",np.shape(tImages), \"Dimension(tLabels)=\", np.shape(tLabels)[0])\n",
        "print(\"--------------------------\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------\n",
            "Dimensions of Train Set\n",
            "Dimension(trImages)= (60000, 28, 28)\n",
            "There are 60000 images where each image is (28, 28) in size\n",
            "There are 10 unique image labels\n",
            "--------------------------\n",
            "Dimensions of Test Set\n",
            "Dimension(tImages)= (10000, 28, 28) Dimension(tLabels)= 10000\n",
            "--------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uKUPyeHCfQU3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##K-Nearest Neighbor Algorithm\n",
        "\n",
        "K-Nearest Neighbor (or KNN) algorithm is a non-parametric classification algorithm. Backprop Neural Network from Part-1 is a parametric model parametrized by weights and bias values. Non-parametric model, contrary to the name, has a very large number of parameters. In the case of Fashion MNIST example, we will use the entire Train Set as parameters of KNN. \n",
        "\n",
        "The basic idea behind KNN is simple. Given a (test) vector or image to classify or label, find $k$ vectors or images in Train Set that are \"closest\" to the (test) vector or image. With the $k$ closest vectors or images, there are $k$ labels. Assign the most frequent label of $k$ labels to the (test) vector or image.\n",
        "\n",
        "###Closeness Metric\n",
        "The idea of \"closest\" or \"closeness\" depends on the metric we choose to use; for instance\n",
        "* **Euclidean Distance** between two vectors $x= <x_1, x_2, x_3>$ and $y= <y_1, y_2, y_3>$ is defined as $d_{ED}:=\\{(x_1-y_1)^2+(x_2-y_2)^2+(x_3-y_3)^2\\}^{\\frac{1}{2}}$. In academic literature, you may see this being called L2 norm of $x-y$.\n",
        "* **L1 Distance** between two vectors  $x= <x_1, x_2, x_3>$ and $y= <y_1, y_2, y_3>$ is defined as $d_{L1}:=|x_1-y_1|+|x_2-y_2|+|x_3-y_3|$\n",
        "* **L0 Distance** between two vectors  $x= <x_1, x_2, x_3>$ and $y= <y_1, y_2, y_3>$ is defined as the number of non-zero elements in $x-y$.\n",
        "\n",
        "In this article, we will use the Euclidean distance and L0 distance. \n",
        "\n",
        "###Finding $k$ Closest Vectors in Train Set\n",
        "Given a vector (or image) from Test Set, we can't say which ones in the Train Set are closest without computing the metric over all elements in the Train Set. In the case of Fashion MNIST, we compute \"closeness\" metric of the vector from Test Set to every element, i.e., 60000 of them, in the Train Set and this will result in 60000 distance values. As you can imagine, if the Train Set is larger then it gets all that more time-consuming or computationally consuming to find all these distance values.\n",
        "\n",
        "###Optimizing Parameter $k$\n",
        "I don't know if there is a systematic way to go about optimizing this parameter but try different \"good\" values for $k$ and pick the one that works best. Let's review some extreme choices for $k$:\n",
        "* If $k=1$, then labeling of the test vector or image is determined by one element in the Train Set\n",
        "* If $k=60000$, then label of the test vector is determined by all elements in the Train Set and if there is class imbalance, i.e., there are more images with a certain label in the Test Set, then every test vector will get the exact same label. "
      ]
    },
    {
      "metadata": {
        "id": "g4TFt0SCdek4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classification using Euclidean Distance Metric\n",
        "\n",
        "Here we use Euclidean distance metric to determine closeness of Train images to a given Test image. In the code snippet below, there are two for-loops, one looping over Test Images and another looping over Train Images. I have set parameter $k=11$; try experimenting with this parameter.\n",
        "\n",
        "The classification accuracy of this metric is poor as you will see from running the code below. For instance, Pullover is classified as a T-Shirt, and a Pant is classified as a Shoe. Overall, the metric is useless and the resultant classification accuracy is poor.\n",
        "\n",
        "### Complexity\n",
        "\n",
        "The complexity of KNN for this example is quite high: for each image in Test Set (there are 10000 of them), we compute 60000 metrics (one each for Train image). After populating an array of 60000 metrics, we scan through this array to identify $k$ smallest metrics."
      ]
    },
    {
      "metadata": {
        "id": "hvjiPiUazcV1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "paramk = 11 # parameter k of k-nearest neighbors\n",
        "numTrainImages = np.shape(trLabels)[0] # so many train images\n",
        "numTestImages = np.shape(tLabels)[0] # so many test images\n",
        "\n",
        "arrayKNNLabels = np.array([])\n",
        "for iTeI in range(1,numTestImages):\n",
        "  arrayL2Norm = np.array([]) # store distance of a test image from all train images\n",
        "  for jTrI in range(numTrainImages):  \n",
        "    l2norm = np.sum(((trImages[jTrI]-tImages[iTeI])/255.0)**2)**(0.5) # distance between two images; 255 is max. pixel value ==> normalization   \n",
        "    arrayL2Norm = np.append(arrayL2Norm, l2norm)\n",
        "    \n",
        "  sIndex = np.argsort(arrayL2Norm) # sorting distance and returning indices that achieves sort\n",
        "  \n",
        "  kLabels = trLabels[sIndex[0:paramk]] # choose first k labels  \n",
        "  (values, counts) = np.unique(kLabels, return_counts=True) # find unique labels and their counts\n",
        "  arrayKNNLabels = np.append(arrayKNNLabels, values[np.argmax(counts)])\n",
        "  print(arrayL2Norm[sIndex[0]], kLabels, arrayKNNLabels[-1], tLabels[iTeI])\n",
        "  \n",
        "  if arrayKNNLabels[-1] != tLabels[iTeI]:\n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.imshow(tImages[iTeI])\n",
        "    plt.draw()\n",
        "    \n",
        "    for i in range(numTrainImages):\n",
        "      if trLabels[i] == arrayKNNLabels[-1]:\n",
        "        plt.figure(2)\n",
        "        plt.imshow(trImages[i])\n",
        "        plt.draw()\n",
        "        break\n",
        "  \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zWzeuu_0UPEE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classification using L0 Distance\n",
        "\n",
        "Here we choose to apply L0 distance to determine closeness of Train images to a given Test image. In the code-snippet below, this is achieved in an indirect way (not an elegant solution) but works nevertheless. For every image (Train or Test), we generate a modified image by assigning $1$ to a pixel that is non-zero in the original image and $0$ to a pixel that is zero in the original image. The modified image is a picture with just 1 and 0. To determine closeness of a Test image and a Train image, we compute Euclidean distance metric between their modified images. This metric is proportional to L0 distance on the original copy of the Test and Train images.\n",
        "\n",
        "Clearly generating a modified image and storing isn't particularly efficient but for purposes of this article it is sufficient. From running the code, I observe that this metric achieves a classification accuracy of **83.8%**, which incidentally isn't too far from what we achieved with a backpropagation network on this dataset.\n",
        "\n",
        "### Complexity\n",
        "\n",
        "The complexity of KNN hasn't changed (much) from changing to L0 distance. Therefore, the above complexity discussion still holds true."
      ]
    },
    {
      "metadata": {
        "id": "TfFce2zpJFuq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Python-like Implementation\n",
        "\n",
        "In the snippet below, we implement the algorithm using Python and Numpy."
      ]
    },
    {
      "metadata": {
        "id": "6UuV2szYMAP9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "paramk = 11 # parameter k of k-nearest neighbors\n",
        "numTrainImages = np.shape(trLabels)[0] # so many train images\n",
        "numTestImages = np.shape(tLabels)[0] # so many test images\n",
        "\n",
        "arrayKNNLabels = np.array([])\n",
        "numErrs = 0\n",
        "for iTeI in range(0,numTestImages):\n",
        "  arrayL2Norm = np.array([]) # store distance of a test image from all train images\n",
        "  \n",
        "  tmpTImage = np.copy(tImages[iTeI])\n",
        "  tmpTImage[tmpTImage > 0] = 1\n",
        "  \n",
        "  for jTrI in range(numTrainImages):\n",
        "    tmpTrImage = np.copy(trImages[jTrI])\n",
        "    tmpTrImage[tmpTrImage>0] = 1\n",
        "    \n",
        "    \n",
        "    l2norm = np.sum(((tmpTrImage-tmpTImage)**2)**(0.5)) # distance between two images; 255 is max. pixel value ==> normalization \n",
        "    if jTrI == 0:\n",
        "      with tf.Session() as sess:\n",
        "        print(tf.count_nonzero(tmpTrImage-tmpTImage, axis=[0,1]).eval())      \n",
        "      print(iTeI, jTrI, l2norm)\n",
        "    arrayL2Norm = np.append(arrayL2Norm, l2norm)\n",
        "    \n",
        "  sIndex = np.argsort(arrayL2Norm) # sorting distance and returning indices that achieves sort\n",
        "  \n",
        "  kLabels = trLabels[sIndex[0:paramk]] # choose first k labels  \n",
        "  (values, counts) = np.unique(kLabels, return_counts=True) # find unique labels and their counts\n",
        "  arrayKNNLabels = np.append(arrayKNNLabels, values[np.argmax(counts)])\n",
        "   \n",
        "  if arrayKNNLabels[-1] != tLabels[iTeI]:\n",
        "    numErrs += 1\n",
        "    print(numErrs,\"/\",iTeI)\n",
        "print(\"# Classification Errors= \", numErrs, \"% accuracy= \", 100.*(numTestImages-numErrs)/numTestImages)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VwhfUWFnJasE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tensorflow-like Implementation\n",
        "\n",
        "The same algorithm is implemented in a fashion that is best suited for Tensorflow. Here we start-off by defining a detailed graph of the algorithm and then, we run it within a Tensorflow instance of a Session. Tensorflow code took me a little figuring out to put together. If you are new to Tensorflow, I recommend going through this piece of code. If readers experienced with the environment, then feel free to suggest code optimizations.\n",
        "\n",
        "I tried running the code with $k=1$ and it achieves classification accuracy of 83.75% which is similar to that achieved with $k=11$."
      ]
    },
    {
      "metadata": {
        "id": "E95EztQIZyl1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "paramk = 11 # parameter k of K-nearest neighbors\n",
        "\n",
        "# Defining KNN Graph with L0 Norm\n",
        "x = tf.placeholder(trImages.dtype, shape=trImages.shape) # all train images, i.e., 60000 x 28 x 28\n",
        "y = tf.placeholder(tImages.dtype, shape=tImages.shape[1:]) # a test image, 28 x 28\n",
        "\n",
        "xThresholded = tf.clip_by_value(tf.cast(x, tf.int32), 0, 1) # x is int8 which is not supported in many tf functions, hence typecast\n",
        "yThresholded = tf.clip_by_value(tf.cast(y, tf.int32), 0, 1) # clip_by_value converts dataset to tensors of 0 and 1, i.e., 1 where tensor is non-zero\n",
        "computeL0Dist = tf.count_nonzero(xThresholded - yThresholded, axis=[1,2]) # Computing L0 Norm by reducing along axes\n",
        "findKClosestTrImages = tf.contrib.framework.argsort(computeL0Dist, direction='ASCENDING') # sorting (image) indices in order of ascending metrics, pick first k in the next step\n",
        "findLabelsKClosestTrImages = tf.gather(trLabels, findKClosestTrImages[0:paramk]) # doing trLabels[findKClosestTrImages[0:paramk]] throws error, hence this workaround\n",
        "findULabels, findIdex, findCounts = tf.unique_with_counts(findLabelsKClosestTrImages) # examine labels of k closest Train images\n",
        "findPredictedLabel = tf.gather(findULabels, tf.argmax(findCounts)) # assign label to test image based on most occurring labels among k closest Train images\n",
        "\n",
        "# Let's run the graph\n",
        "numErrs = 0\n",
        "numTestImages = np.shape(tLabels)[0]\n",
        "numTrainImages = np.shape(trLabels)[0] # so many train images\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  for iTeI in range(0,numTestImages): # iterate each image in test set\n",
        "    predictedLabel = sess.run([findPredictedLabel], feed_dict={x:trImages, y:tImages[iTeI]})   \n",
        "\n",
        "    if predictedLabel == tLabels[iTeI]:\n",
        "      numErrs += 1\n",
        "      print(numErrs,\"/\",iTeI)\n",
        "      print(\"\\t\\t\", predictedLabel[0], \"\\t\\t\\t\\t\", tLabels[iTeI])\n",
        "      \n",
        "      if (1):\n",
        "        plt.figure(1)\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.imshow(tImages[iTeI])\n",
        "        plt.title('Test Image has label %i' %(predictedLabel[0]))\n",
        "        \n",
        "        for i in range(numTrainImages):\n",
        "          if trLabels[i] == predictedLabel:\n",
        "            plt.subplot(1,2,2)\n",
        "            plt.imshow(trImages[i])\n",
        "            plt.title('Correctly Labeled as %i' %(tLabels[iTeI]))\n",
        "            plt.draw()\n",
        "            break\n",
        "        plt.show()\n",
        "\n",
        "print(\"# Classification Errors= \", numErrs, \"% accuracy= \", 100.*(numTestImages-numErrs)/numTestImages)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}